\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{tabularx}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{mathtools}
\graphicspath{ {./imgs/} }

\usepackage{amssymb,latexsym,amsmath}
\allowdisplaybreaks
\usepackage[style=ieee]{biblatex}
\addbibresource{LiamBibliography.bib}

\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\algsetup{linenodelimiter=\ }
\renewcommand{\algorithmiccomment}[1]{(#1)}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]

\allowdisplaybreaks

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\title{Zero-Delay Coding over a Noisy Channel
    %    \thanks{Research is supported by the Natural Sciences and Engineering Research Council of Canada. The first author was supported by a Queen's University Department of Mathematics and Statistics Summer Research Award.}
}

%\author{\IEEEauthorblockN{Liam Cregg}
%    \IEEEauthorblockA{\textit{Mathematics and Statistics} \\
%        \textit{Queen's University}\\
%        Kingston, Canada \\
%        liam.cregg@queensu.ca}
%    \and
%    \IEEEauthorblockN{Tam\'as Linder}
%    \IEEEauthorblockA{\textit{Mathematics and Statistics} \\
%        \textit{Queen's University}\\
%        Kingston, Canada \\
%        tamas.linder@queensu.ca}
%    \and
%    \IEEEauthorblockN{Serdar Y\"uksel}
%    \IEEEauthorblockA{\textit{Mathematics and Statistics} \\
%        \textit{Queen's University}\\
%        Kingston, Canada \\
%        yuksel@queensu.ca}}

\maketitle

%\begin{abstract}
%    In the classical lossy coding problem, one is allowed to encode long sequences of source symbols in order to achieve a lower distortion. This is undesirable in many delay-sensitive applications. Accordingly, we consider the zero-delay case where one wishes to encode a source symbol causally. It has been shown that this problem lends itself to stochastic control techniques, leading to existence and structural results. However, attempting to develop analytical solutions or algorithmic implementation using these methods has been computationally difficult. To that end, we propose a reinforcement learning approach. First, we will establish some supporting results on regularity and stability properties. Afterwards, building on recent results on quantized Q-learning, we show that a quantized Q-learning algorithm can be used to obtain a near-optimal policy for this problem. Finally, we provide some relevant simulation results.
%\end{abstract}

% Intro to zero-delay quantizers, summarize results from "Zero-Delay Lossy Coding"
\section{Zero-Delay Lossy Coding: Noisy Channel Case}\label{section:optimal quantizers}
\subsection{Optimal Quantizers}
We will assume throughout that the source \( \{X_t\}_{t \ge 0} \) is a discrete-time Markov process with probability matrix \( P \), which is irreducible and aperiodic (and thus admits a unique invariant measure). After encoding, the (compressed) information is sent over a discrete memoryless \emph{noisy} channel with input alphabet \( \mathcal{M} := \{1,\ldots,M\} \) and output alphabet \(\mathcal{M}' := \{1, \ldots, M'\}\). We assume the encoder has access to feedback from previous channel outputs.

Thus, the encoder is defined by an encoder policy \( \{\gamma^e_t\}_{t \ge 0} \), where \( \gamma^e_t : (\mathcal{M'})^t \times \mathbb{X}^{t+1} \to \mathcal{M} \). That is, the encoder can use all past channel outputs and all past and current source inputs to generate the current encoder output. This can be viewed as the encoder policy selecting a quantizer \( Q_t : \mathbb{X} \to \mathcal{M} \) using past information, then quantizing \( X_t \) as \( q_t = Q_t(X_t) \)~\cite{Linder}. Then \(q_t\) is sent over the channel, and the channel output \(q'_t\) is determined by the transition probability \(T(q'_t | q_t)\). The decoder generates the reconstruction \( \hat{X}_t \) without delay, using decoder policy \( \{\gamma^d_t\}_{t \ge 0} \), where \( \gamma^d_t : (\mathcal{M'})^{t+1} \to \hat{\mathbb{X}} \). Thus we have \( \hat{X}_t = \gamma^d_t(q'_{[0,t]}) \). We now present some noisy-channel analogs to the original noiseless case.

In a similar fashion to the noiseless case, one can restrict to optimal encoders and assume the optimal corresponding decoder is used. We then consider the discounted cost problem; for some \( \beta \in (0,1) \), we wish to minimize:
\[ J_{\beta}(\mu, \gamma) := \lim_{T\to\infty}\mathbf{E}_{\mu}^{\gamma}\left[\frac{1}{T}\sum_{t=0}^{T-1}\beta^t d(X_t,\hat{X}_t)\right]\label{eq:discounted_cost} \]

For finite horizon problems, we have the following results on the structure of optimal zero-delay codes, the first from Witsenhausen and the second from Walrand and Varaiya (which were later generalized further in the literature, see e.g. \cite{MahTen09,Teneketzis,YukIT2010arXiv}):

\begin{theorem}\cite{Witsenhausen}
    For the problem of coding a Markov source over a finite time horizon \(T\), any zero delay encoder policy \(\gamma=\{\gamma_t\}\) can be replaced, without loss in distortion performance, by a policy \(\hat{\gamma}=\{\hat{\gamma_t}\}\) which only uses \(q'_{[0,t-1]}\) and \(X_t\) to generate \(q_t\), i.e., such that \(q_t = \hat{\gamma}_t(q'_{[0,t-1]}, X_t) \) for all \( t = 1,\ldots,T-1\).
\end{theorem}

We define the following conditional probability measure on \(\mathbb{X}\). Let \(\mathcal{P}(\mathbb{X})\) be the space of probability measures on \(\mathbb{X}\), and define \(\pi_t \in \mathcal{P}(\mathbb{X})\) as:
\[\pi_t(A) := \text{Pr}(X_t \in A | q'_{[0,t-1]})\]
\begin{theorem}\label{theorem:Walrand}\cite{Walrand}
    For the problem of coding a Markov source over a finite time horizon \(T\), any zero delay encoder policy \(\gamma=\{\gamma_t\}\) can be replaced, without loss in distortion performance, by a policy \(\hat{\gamma}=\{\hat{\gamma_t}\}\) which only uses \(\pi_t\) and \(X_t\) to generate \(q_t\), i.e., such that \(q_t = \hat{\gamma}_t(\pi_t, X_t) \) for all \( t = 1,\ldots,T-1\). Alternatively, at time \(t\) such a policy uses \(\pi_t\) to select a quantizer \(Q_t = \hat{\gamma}_t(\pi_t)\) (where \(Q_t : \mathbb{X} \to \mathcal{M}\)), and then \(q_t\) is obtained by \(q_t = Q_t(X_t)\).
\end{theorem}

Encoders with the above structure are called \emph{Walrand-Varaiya type} policies in~\cite{Linder,Wood}, or alternatively, Markov policies (because they endow Markov properties onto the process \( \{ \pi_t \} \), which we will see momentarily). We also have an infinite-horizon analog of the above theorem, which was proven in~\cite{Wood}:

\begin{theorem}\label{theorem:Wood}\cite[Theorem 7]{Wood}
    For the problem of coding an irreducible and aperiodic Markov source, for any initial distribution \( \mu \), there exists a stationary Walrand-Varaiya type policy \(\gamma^*\) that solves the infinite-horizon discounted cost problem, i.e. one that satisfies:
    \[J_\beta(\mu,\gamma^*) = \inf_{\gamma \in \Gamma}J_\beta(\mu,\gamma)\]
    where \( \Gamma \) is the set of all admissible encoder policies and \(J_\beta(\mu,\gamma)\) is defined in~\eqref{eq:discounted_cost}.
\end{theorem}

Under Walrand-Varaiya type policies, it was shown in~\cite{Linder} that \( \{\pi_t\} \) is a controlled Markov process with control \( \{Q_t\} \). More specifically, we have the following result:
\begin{theorem}\cite{Linder}
    Under a Walrand-Varaiya type policy, the update equation for \(\pi_t\) is given by
    \begin{equation}
        \pi_{t+1}(x_{t+1}) = \frac{\sum_{q_t}T(q'_t|q_t) \smashoperator[lr]{\sum_{x_t \in Q_t^{-1}(q_t)}}P(x_{t+1}|x_t)\pi_t(x_t)}{\sum_{q_t}T(q'_t|q_t)\pi_t(Q_t^{-1}(q_t))}\label{eq:1}
    \end{equation}
    Therefore \( \pi_{t+1} \) is conditionally independent of \( (\pi_{[0,t-1]}, Q_{[0,t-1]}) \) given \( \pi_t \) and \( Q_t \), and hence \( \{\pi_t\} \) is a controlled Markov process with control \( \{Q_t\} \).
\end{theorem}

And we define the following cost function for this Markov decision process (MDP) in terms of \( \pi_t \) and \( Q_t \) (this is the average distortion if the optimal decoder is used for a given \(Q_t\)).

\begin{equation}\label{eq:cost}
    c(\pi_t, Q_t) := \sum_{i=1}^{M} \min_{\hat{x} \in \hat{\mathbb{X}}} \sum_{x \in Q_t^{-1}(i)} \pi_t(x)d(x,\hat{x})
\end{equation}

Note that by this definition of \( c(\pi_t,Q_t) \) and our assumption that we are using an optimal decoder for a given encoder of the optimal Walrand-Varaiya type, we have:
\[\mathbf{E}_{\mu}^{\gamma}\left[\frac{1}{T}\sum_{t=0}^{T-1}c(\pi_t,Q_t)\right] = \mathbf{E}_{\mu}^{\gamma}\left[\frac{1}{T}\sum_{t=0}^{T-1}d(X_t,\hat{X}_t)\right]\]

% Motivation: value iterations etc. is very difficult (not just computationally but also algorithm design, given the setup of problem)
%In theory one could use dynamic programming principles to run an iterative algorithm on this controlled Markov process in order to obtain the optimal policy. However, in practice this proves to be difficult (see e.g.~\cite{Wood}), and hence we propose to use Q-learning in order to find the optimal quantization policy. To this end, we will utilize the recent work of~\cite{Kara} in the near-optimality of policies obtained through Q-learning under quantization.

\subsection{A Topology on Quantizers}
This is defined exactly as in the noiseless case (i.e. \(Q_n \to Q\) weakly iff \(PQ_n \to PQ\) weakly). Under this topology, the results in~\cite{Linder} can be extended to show the following property of the controlled Markov chain \( \{\pi_t\} \).

\begin{lemma}\label{lemma:weak}~\cite[Lemma 11]{Linder}.
    The transition kernel \( P(d\pi_{t+1}|\pi_t,Q_t) \) is weakly continuous in \( (\pi_t,Q_t) \). That is,

    \[ \int_{\mathcal{P}(\mathbb{X}) \times \mathcal{Q}} f(\pi^{'})P(d \pi^{'}|\pi, Q) \]

    is continuous on \( \mathcal{P}(\mathbb{X}) \times \mathcal{Q} \) for all continuous bounded \(f\).
\end{lemma}


% Intro to quantized Q-learning, summarize results from "Q-learning for General Spaces"
\section{Q-learning and Quantized Q-learning}\label{section:Q-learning}
This section applies to a general MDP and is not specific to the noisy/noiseless case, so it has been omitted. We again note that the only non-trivial assumption from~\cite[Theorem 3.2]{Kara} that we must prove is the following.
\begin{assumption}
    Under a random exploration policy \(\gamma\), the process \( \{ \pi_t \}_{t\ge0} \) admits a unique invariant measure.
\end{assumption}

Where a memoryless exploration policy \( \gamma \) is one in which
\[ \textup{Pr}(\gamma(\cdot) = u_i) = p_i \quad \forall i = 1,\ldots,|\mathbb{U}| \]
where \(p_i > 0 \; \forall i\) and \(\sum_i p_i = 1\).

\section{Unique Ergodicity Under a Memoryless Exploration Policy}\label{section:unique-ergodicity}
Recall our setup from Section~\ref{section:optimal quantizers}. In particular, we have a controlled Markov process \(\{\pi_t\}\), with control \(\{Q_t\}\), where \(Q_t : \mathbb{X} \to \mathcal{M}\). Here \(\mathbb{X}\) is our source alphabet, \(\mathcal{M}\) is our message set, we have \(Q_t(X_t) = q_t\), and \(q'_t\) is obtained through \(T(q'_t | q_t)\).

We wish to show that if we choose the \(Q_t\) randomly, \( \{\pi_t\} \) admits a unique invariant measure. In order to apply the same POMDP methods we used in the noiseless case, we will note that the conditional probability of \(q'_t\) given \(x_t\) is determined by \(T\) and the distribution of \(Q_t\). We denote the resulting measurement kernel by \(S(q'_t | x_t) = \sum_{q \in \mathcal{M}}T(q'_t|q)R(\{Q \in \mathcal{S} : Q(x_t)=q\})\), where (as in the noiseless case) we define \(R\) as a distribution on the set of quantizers \(S\) according to our random exploration policy (that is, the distribution of \(Q_t\) is described entirely by \(R\), which is positive everywhere).

As we did in the noiseless case, we have unique ergodicity of the process \(\{\pi_t\}_{t\ge0}\) if we can show that the \emph{filter} \(\pi^*_t(A) := \text{Pr}(X_t \in A | q'_{[0,t]})\) is stable (in total variation in expectation). To this end, we impose the same assumption on our set of quantizers \(\mathcal{S}\) as in the noiseless case:

\begin{assumption}\label{assumption:one-bin}
    For all \(x \in \mathbb{X} \) and for all \(q \in \mathcal{M} \) we have \(\{Q \in \mathcal{S} : Q(x) = q\} \neq \emptyset \).
\end{assumption}

Then, as in the noiseless case, we have that \(R(\{Q \in \mathcal{S} : Q(x)=q\})\) is positive . But then this implies that \(S(q'|x)\) is positive everywhere (if not, we must have that \(T(q'|q)=0\) for all \(q\), so \(q'\) is not a valid channel output).

Then, filter stability follows from~\cite[Corollary 5.5]{Handel}, and so \( \{\pi_t\}_{t\ge0} \) admits a unique invariant measure. Therefore all the assumptions for quantized Q-learning to converge are met. The slightly modified algorithms are below.

\section{Algorithms}\label{section:Algorithms}
\subsection{Quantizing \( \pi_t \)}\label{algorithm1}
Since the state space \( \mathbb{X} \) is finite, say with \( |\mathbb{X}| = m \), then \( \mathcal{P}(\mathbb{X}) \) is a simplex in \( \mathbb{R}^m \). For a given belief \( \pi_t \) and \( n \), we wish to find the nearest (in terms of Eucilidean distance) \( \hat{\pi}_t = [\frac{k_1}{n}, \ldots, \frac{k_m}{n}] \), where \( k_i \in \mathbb{Z} \). Then we can use the algorithm in e.g. \cite{Reznik}, \cite{Saldi} to quantize \(\pi_t\) as follows. %chktex 2

\vspace{1em}

\noindent \underline{\textbf{Algorithm 1: Predictor Quantization}}

\begin{algorithmic}[1]
    \REQUIRE \( n\ge1, \pi_t = (p_1, \ldots, p_m) \)
    \FOR{\(i=1\) \TO \(m\)}
    \STATE \(k_i' = \lfloor np_i + \frac{1}{2} \rfloor\)
    \ENDFOR
    \STATE \(n' = \sum_i k_i'\)
    \IF{\(n=n'\)}
    \RETURN \((\frac{k_1'}{n}, \ldots, \frac{k_m'}{n})\)
    \ENDIF
    \FOR{\(i=1\) \TO \(m\)}
    \STATE \( \delta_i = k_i' - np_i\)
    \ENDFOR
    \STATE \textbf{Sort} \( \delta_i \) s.t. \( \delta_{i_1} \le \ldots \le \delta_{i_m} \)
    \STATE \( \Delta = n'-n \)
    \IF{\( \Delta > 0 \)}
    \STATE \(k_{i_j} = \begin{cases}
        k_{i_j}' \quad   & j = 1,\ldots,m-\Delta   \\
        k_{i_j}'-1 \quad & j = m-\Delta+1,\ldots,m
    \end{cases}\)
    \ELSE
    \STATE \(k_{i_j} = \begin{cases} k_{i_j}'+1 \quad & j = 1,\ldots,|\Delta| \\ k_{i_j}' \quad   & j = |\Delta|+1,\ldots,m \end{cases}\)
    \ENDIF
    \RETURN \((\frac{k_1'}{n}, \ldots, \frac{k_m'}{n}) \)
\end{algorithmic}

We have the following lemma regarding the radius of these quantization bins under the above algorithm.

\begin{lemma}\label{lemma:radius}~\cite[Proposition 2]{Reznik}
    The maximum radius of the quantization regions for \(\pi_t\) under the \( L_{\infty} \) norm is given by
    \[ b_{\infty} = \frac{1}{n}(1-\frac{1}{m}) \]
\end{lemma}

Also note that the number of bins for \( \pi_t \) when using \textbf{Algorithm 1} is related to \( n \) by the following relation: \( \text{\# bins} = {{n+m-1} \choose {m-1}} \)~\cite{Reznik}.

\subsection{Quantized Q-learning}\label{algorithm2}
% Convergence proof for finite case
Using the above algorithm to quantize \( \pi_t \), we have the following algorithm for quantized Q-learning.
\newpage

\vspace{1em}

\noindent \underline{\textbf{Algorithm 2: Quantized Q-learning}}

\begin{algorithmic}[1]
    \REQUIRE source alphabet \(\mathbb{X}\), transition kernel \(P(x_{t+1} | x_t)\), initial distribution \(\pi_0\), quantization parameter \(n\), quantizer set \(\mathcal{Q}\), exploration policy \(\gamma\), time horizon \(T\)
    \STATE Initialize Q-table of size \( {{n+m-1} \choose {m-1}} \times |\mathcal{Q}| \)
    \STATE Initialize \( x_0 \) according to \( \pi_0 \)
    \STATE Quantize \( \pi_0 \) using \textbf{Algorithm 1}, call this \( \hat{\pi}_0 \)
    \STATE Select quantizer \( Q_0 \) according to \( \gamma \)
    \STATE \( q_0 = Q_0(x_0) \)
    \FOR{\(t=0\) \TO \(T-1\)}
    \STATE Compute \(c(\pi_t, Q_t)\) \COMMENT{see~\eqref{eq:cost}}
    \STATE Receive \( x_{t+1} \) according to \( P(x_{t+1} | x_t) \)
    \STATE Receive \( \pi_{t+1} \) according to update equation \COMMENT{see~\eqref{eq:1}}
    \STATE Quantize \( \pi_{t+1} \) using \textbf{Algorithm 1}, call this \( \hat{\pi}_{t+1} \)
    \STATE Update Q-table
    \STATE Select quantizer \( Q_{t+1} \) according to \( \gamma \)
    \STATE \( q_{t+1} = Q_{t+1}(x_{t+1}) \)
    \STATE Receive \(q'_t\) according to \(T(q'_t | q_t)\)
    \ENDFOR
    \RETURN \(\gamma^*(\pi) = \argmin_{Q \in \mathcal{Q}}\)(Q-table\((\pi,Q)\))
\end{algorithmic}

\begin{theorem}
    Under Assumption~\ref{assumption:one-bin} and as \(n \to \infty\), the above algorithm gives a near-optimal policy for the zero-delay coding problem.
\end{theorem}
\begin{proof}
    As in the noiseless case, the proof follows from the fact that \( \{\pi_t\} \) admits a unique invariant measure and from~\cite[Theorem 3.2]{Kara}.
\end{proof}

\section{Examples}
Currently filling out this section with simulations. Unfortunately, they take a while longer than the noiseless case (presumably due to the extra complexity of computing \(\pi_t\)). Preliminary results look similar to the noiseless case, but the benefits of finer quantization are generally less obvious.

\printbibliography
\end{document} %chktex 17